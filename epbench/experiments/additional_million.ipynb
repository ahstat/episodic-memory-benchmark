{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_filepath = '/filepath/to/gitrepo/episodic-memory-benchmark'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, 20.00% remaining with issues (4/20), for index: [11, 13, 16, 19].\n",
      "At iteration 1, 15.00% remaining with issues (3/20), for index: [11, 13, 16].\n",
      "At iteration 2, 10.00% remaining with issues (2/20), for index: [13, 16].\n",
      "At iteration 3, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 4, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 5, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 6, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 7, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 8, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At final iteration 9, 5.00% remaining with issues (1/20), for index: [16].\n",
      "itermax reached but some events still did not pass the verification\n",
      "At iteration 0, 33.50% remaining with issues (67/200), for index: [11, 13, 16, 19, 20, 23, 25, 30, 33, 42, 44, 45, 47, 48, 50, 51, 56, 59, 62, 63, 67, 69, 70, 71, 79, 80, 85, 86, 88, 93, 96, 106, 109, 122, 125, 127, 128, 130, 136, 138, 143, 144, 146, 147, 148, 149, 150, 152, 155, 156, 160, 162, 163, 166, 169, 172, 175, 177, 178, 180, 181, 182, 185, 189, 193, 197, 199].\n",
      "At iteration 1, 16.50% remaining with issues (33/200), for index: [11, 13, 16, 42, 44, 56, 59, 67, 79, 80, 93, 96, 106, 122, 127, 128, 130, 136, 143, 144, 146, 147, 150, 156, 160, 162, 163, 166, 169, 172, 175, 182, 193].\n",
      "At iteration 2, 10.50% remaining with issues (21/200), for index: [13, 16, 42, 44, 56, 67, 79, 93, 96, 106, 143, 144, 146, 150, 156, 160, 162, 166, 169, 182, 193].\n",
      "At iteration 3, 7.50% remaining with issues (15/200), for index: [16, 42, 44, 56, 67, 93, 96, 106, 143, 144, 146, 156, 160, 182, 193].\n",
      "At iteration 4, 5.50% remaining with issues (11/200), for index: [16, 42, 44, 56, 67, 93, 146, 156, 160, 182, 193].\n",
      "At iteration 5, 4.50% remaining with issues (9/200), for index: [16, 56, 67, 93, 146, 156, 160, 182, 193].\n",
      "At iteration 6, 3.00% remaining with issues (6/200), for index: [16, 56, 67, 156, 160, 182].\n",
      "At iteration 7, 2.50% remaining with issues (5/200), for index: [16, 56, 67, 156, 160].\n",
      "At iteration 8, 2.00% remaining with issues (4/200), for index: [16, 56, 156, 160].\n",
      "At final iteration 9, 2.00% remaining with issues (4/200), for index: [16, 56, 156, 160].\n",
      "itermax reached but some events still did not pass the verification\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from epbench.src.generation.benchmark_generation_wrapper import BenchmarkGenerationWrapper\n",
    "book_parameters = {'indexing': 'default', 'nb_summaries': 0}\n",
    "data_folder = Path(git_repo_filepath) / 'epbench' / 'data'\n",
    "env_file = Path(git_repo_filepath) / '.env'\n",
    "\n",
    "# Generation with Claude -- 20 events\n",
    "prompt_parameters = {'nb_events': 20, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_20 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "# Generation with Claude -- 200 events\n",
    "prompt_parameters = {'nb_events': 200, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_200 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "# Generation with Claude -- 2000 events\n",
    "import pickle\n",
    "precomputed_million_book_repository = data_folder / 'benchmark_claude_2000.pkl'\n",
    "if not precomputed_million_book_repository.exists():\n",
    "    prompt_parameters = {'nb_events': 2000, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "    model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "    benchmark_claude_2000 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file, rechecking = False)\n",
    "    with open(precomputed_million_book_repository, 'wb') as f: # save the object\n",
    "        pickle.dump(benchmark_claude_2000, f)\n",
    "with open(precomputed_million_book_repository, 'rb') as f:\n",
    "    benchmark_claude_2000 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional experiments (in-context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with 1033475 tokens, answer with prompting using with gemini-2.5-pro\n",
      "Experiment ended (prompting)\n"
     ]
    }
   ],
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "\n",
    "for my_benchmark in [benchmark_claude_2000]:\n",
    "    for model_name in ['gemini-2.5-pro']:\n",
    "        answering_parameters = {'kind': 'prompting', 'model_name': model_name, 'max_new_tokens': 4096, 'sleeping_time': 1, 'policy': 'remove_duplicates'}\n",
    "        print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with prompting using with {model_name}\")\n",
    "        my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Experiment ended (prompting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading experiments (in-context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 experiments\n",
      "Document with 1033475 tokens, answer with prompting using with gemini-2.5-pro\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_nb_events</th>\n",
       "      <th>answering_kind</th>\n",
       "      <th>answering_model_name</th>\n",
       "      <th>answering_embedding_chunk</th>\n",
       "      <th>book_model_name</th>\n",
       "      <th>evaluation_object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000</td>\n",
       "      <td>prompting</td>\n",
       "      <td>gemini-2.5-pro</td>\n",
       "      <td>n/a</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_nb_events answering_kind answering_model_name  \\\n",
       "0            2000      prompting       gemini-2.5-pro   \n",
       "\n",
       "  answering_embedding_chunk             book_model_name  \\\n",
       "0                       n/a  claude-3-5-sonnet-20240620   \n",
       "\n",
       "                                   evaluation_object  \n",
       "0  <epbench.src.evaluation.evaluation_wrapper.Eva...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "from epbench.src.evaluation.precomputed_results import get_precomputed_results\n",
    "\n",
    "experiments = [\n",
    "    # in-context, book with 2000 events\n",
    "    {'book_nb_events': 2000,  'answering_kind': 'prompting', 'answering_model_name': 'gemini-2.5-pro'}\n",
    "]\n",
    "\n",
    "for i in range(len(experiments)):\n",
    "    if not 'answering_embedding_chunk' in experiments[i]:\n",
    "        experiments[i]['answering_embedding_chunk'] = 'n/a'\n",
    "    experiments[i]['book_model_name'] = 'claude-3-5-sonnet-20240620'\n",
    "\n",
    "print(f\"{len(experiments)} experiments\")\n",
    "\n",
    "all_benchmarks = {'benchmark_claude_default_20': benchmark_claude_20,\n",
    "                  'benchmark_claude_default_200': benchmark_claude_200,\n",
    "                  'benchmark_claude_default_2000': benchmark_claude_2000}\n",
    "\n",
    "df = get_precomputed_results(experiments, env_file, data_folder, all_benchmarks)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manual check for one incorrect event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the text provided, Andrew Gomez was not involved in any events related to a Fashion Show.\n",
      "\n",
      "The AI-generated answer does not provide any information about spaces or locations. Instead, it states that Andrew Gomez was not involved in any events related to a Fashion Show, which is not relevant to the question type about spaces. The groundtruth item 'Museum of Modern Art' is not mentioned or alluded to in any way, resulting in a score of 0.\n",
      "\n",
      "Chapter 1624\n",
      "The shimmering, ethereal fabric cascaded down the runway like liquid starlight, catching the eye of Andrew Gomez as \n",
      "he leaned forward in his seat. His fingers itched to touch the gossamer-thin material, to feel its magic pulse beneath \n",
      "his skin. The air around him thrummed with anticipation, a heady mix of excitement and wonder that only a fashion show \n",
      "of this caliber could evoke.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bins_items_correct_answer                                                           1\n",
       "correct_answer_chapters                                                        [1624]\n",
       "get                                                                               all\n",
       "n_items_correct_answer                                                              1\n",
       "llm_answer                          Based on the text provided, Andrew Gomez was n...\n",
       "predicted_items                                                                    []\n",
       "matching_groundtruth_items_score                        [{'Museum of Modern Art': 0}]\n",
       "explanation                         The AI-generated answer does not provide any i...\n",
       "f1_score_lenient                                                                  0.0\n",
       "Name: 4, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = df['evaluation_object'].iloc[0].df_generated_evaluations # 0 is gemini-2.5-pro for now\n",
    "e = e[['bins_items_correct_answer', 'correct_answer_chapters',\n",
    "       'get',\n",
    "       'n_items_correct_answer', 'llm_answer', 'predicted_items', 'matching_groundtruth_items_score', 'explanation',\n",
    "       'f1_score_lenient']]\n",
    "\n",
    "k = 4 # checked manually this one\n",
    "print(e.iloc[k]['llm_answer'])\n",
    "print(\"\")\n",
    "print(e.iloc[k]['explanation'])\n",
    "#e.columns\n",
    "\n",
    "one_chapter_ok = e.iloc[k]['correct_answer_chapters']\n",
    "if len(one_chapter_ok) > 0:\n",
    "    one_chapter_ok = one_chapter_ok[0]\n",
    "else:\n",
    "    print('no chapter groundtruth')\n",
    "\n",
    "print(\"\")\n",
    "# print(benchmark_claude_2000.chunk_book('chapter')[one_chapter_ok-1])\n",
    "print(\"\"\"Chapter 1624\n",
    "The shimmering, ethereal fabric cascaded down the runway like liquid starlight, catching the eye of Andrew Gomez as \n",
    "he leaned forward in his seat. His fingers itched to touch the gossamer-thin material, to feel its magic pulse beneath \n",
    "his skin. The air around him thrummed with anticipation, a heady mix of excitement and wonder that only a fashion show \n",
    "of this caliber could evoke.\"\"\")\n",
    "e.iloc[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bins_items_correct_answer</th>\n",
       "      <th>count</th>\n",
       "      <th>(prompting, gemini-2.5-pro, n/a)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00¬±0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.33¬±0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.72¬±0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3-5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.70¬±0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6+</td>\n",
       "      <td>9</td>\n",
       "      <td>0.52¬±0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  bins_items_correct_answer  count (prompting, gemini-2.5-pro, n/a)\n",
       "0                         0      3                        1.00¬±0.00\n",
       "1                         1      3                        0.33¬±0.58\n",
       "2                         2      3                        0.72¬±0.25\n",
       "3                       3-5      3                        0.70¬±0.17\n",
       "4                        6+      9                        0.52¬±0.18"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epbench.src.evaluation.ranking import get_simple_results\n",
    "nb_events = 2000 # select the book of interest\n",
    "df_results_simple = get_simple_results(df, nb_events)\n",
    "df_results_simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarizing table column \"Simple Recall\" in the github (updated with the latest changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(prompting, gemini-2.5-pro, n/a)    0.654\n",
       "dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epbench.src.evaluation.ranking import simple_recall_score\n",
    "simple_recall_score(df_results_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chronological score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gemini-2.5-pro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Latest</th>\n",
       "      <td>64.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kendall œÑ</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gemini-2.5-pro\n",
       "Latest            64.29%\n",
       "All                 0.0%\n",
       "Kendall œÑ            NaN"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epbench.src.evaluation.ranking import get_kendall_tau_results\n",
    "nb_events = 2000\n",
    "kendall_tau_results = get_kendall_tau_results(df, nb_events)\n",
    "kendall_tau_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summarizing table column \"Chronological Awareness\" in the github (updated with the latest changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gemini-2.5-pro    0.321\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epbench.src.evaluation.ranking import chronological_awareness\n",
    "chronological_awareness(kendall_tau_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct computation of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>üéØ Simple Recall (million token book)</th>\n",
       "      <th>‚è±Ô∏è Chronological Awareness (million token book)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gemini-2.5-pro</th>\n",
       "      <td>0.654</td>\n",
       "      <td>0.321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                üéØ Simple Recall (million token book)  \\\n",
       "gemini-2.5-pro                                 0.654   \n",
       "\n",
       "                ‚è±Ô∏è Chronological Awareness (million token book)  \n",
       "gemini-2.5-pro                                            0.321  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from epbench.src.evaluation.ranking import get_final_scores_table\n",
    "nb_events = 2000 # select the book of interest\n",
    "get_final_scores_table(df, nb_events)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
