{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_filepath = '/filepath/to/gitrepo/episodic-memory-benchmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default book -- Generation with GPT -- 20 targeted events (finally 20 chapters and 14k tokens)\n",
      "At iteration 0, 50.00% remaining with issues (10/20), for index: [0, 1, 2, 4, 7, 8, 11, 12, 16, 17].\n",
      "At iteration 1, 30.00% remaining with issues (6/20), for index: [0, 1, 4, 7, 8, 11].\n",
      "At iteration 2, 20.00% remaining with issues (4/20), for index: [0, 7, 8, 11].\n",
      "At iteration 3, 15.00% remaining with issues (3/20), for index: [0, 7, 8].\n",
      "At iteration 4, 15.00% remaining with issues (3/20), for index: [0, 7, 8].\n",
      "At iteration 5, 15.00% remaining with issues (3/20), for index: [0, 7, 8].\n",
      "At iteration 6, 10.00% remaining with issues (2/20), for index: [0, 7].\n",
      "At iteration 7, 10.00% remaining with issues (2/20), for index: [0, 7].\n",
      "At iteration 8, 10.00% remaining with issues (2/20), for index: [0, 7].\n",
      "At final iteration 9, 0.00% remaining with issues (0/20), for index: [].\n",
      "Default book -- Generation with Claude -- 20 targeted events\n",
      "At iteration 0, 20.00% remaining with issues (4/20), for index: [11, 13, 16, 19].\n",
      "At iteration 1, 15.00% remaining with issues (3/20), for index: [11, 13, 16].\n",
      "At iteration 2, 10.00% remaining with issues (2/20), for index: [13, 16].\n",
      "At iteration 3, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 4, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 5, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 6, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 7, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 8, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At final iteration 9, 5.00% remaining with issues (1/20), for index: [16].\n",
      "itermax reached but some events still did not pass the verification\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from epbench.src.generation.benchmark_generation_wrapper import BenchmarkGenerationWrapper\n",
    "book_parameters = {'indexing': 'default', 'nb_summaries': 0}\n",
    "data_folder = Path(git_repo_filepath) / 'epbench' / 'data'\n",
    "env_file = Path(git_repo_filepath) / '.env'\n",
    "\n",
    "print(\"Default book -- Generation with GPT -- 20 targeted events (finally 20 chapters and 14k tokens)\")\n",
    "prompt_parameters = {'nb_events': 20, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'gpt-4o-2024-05-13', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_gpt_default_20 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Default book -- Generation with Claude -- 20 targeted events\")\n",
    "prompt_parameters = {'nb_events': 20, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_default_20 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with 13680 tokens, answer with prompting using with gpt-4o-mini-2024-07-18\n",
      "Document with 13680 tokens, answer with prompting using with gpt-4o-2024-08-06\n",
      "Document with 13680 tokens, answer with prompting using with claude-3-haiku-20240307\n",
      "Document with 13680 tokens, answer with prompting using with claude-3-5-sonnet-20240620\n",
      "Experiment ended (prompting)\n"
     ]
    }
   ],
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "\n",
    "for my_benchmark in [benchmark_gpt_default_20]:\n",
    "    for model_name in ['gpt-4o-mini-2024-07-18', 'gpt-4o-2024-08-06', 'claude-3-haiku-20240307', 'claude-3-5-sonnet-20240620']: # discard the costly 'o1-mini'\n",
    "        answering_parameters = {'kind': 'prompting', 'model_name': model_name, 'max_new_tokens': 4096, 'sleeping_time': 1, 'policy': 'remove_duplicates'}\n",
    "        print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with prompting using with {model_name}\")\n",
    "        my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Experiment ended (prompting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 experiments\n",
      "Document with 10397 tokens, answer with prompting using with gpt-4o-mini-2024-07-18\n",
      "Document with 10397 tokens, answer with prompting using with gpt-4o-2024-08-06\n",
      "Document with 10397 tokens, answer with prompting using with claude-3-haiku-20240307\n",
      "Document with 10397 tokens, answer with prompting using with claude-3-5-sonnet-20240620\n",
      "Document with 13680 tokens, answer with prompting using with gpt-4o-mini-2024-07-18\n",
      "Document with 13680 tokens, answer with prompting using with gpt-4o-2024-08-06\n",
      "Document with 13680 tokens, answer with prompting using with claude-3-haiku-20240307\n",
      "Document with 13680 tokens, answer with prompting using with claude-3-5-sonnet-20240620\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_nb_events</th>\n",
       "      <th>answering_kind</th>\n",
       "      <th>answering_model_name</th>\n",
       "      <th>book_model_name</th>\n",
       "      <th>answering_embedding_chunk</th>\n",
       "      <th>evaluation_object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>gpt-4o-mini-2024-07-18</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>claude-3-haiku-20240307</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>gpt-4o-mini-2024-07-18</td>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>gpt-4o-2024-08-06</td>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>claude-3-haiku-20240307</td>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>prompting</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>gpt-4o-2024-05-13</td>\n",
       "      <td>n/a</td>\n",
       "      <td>&lt;epbench.src.evaluation.evaluation_wrapper.Eva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_nb_events answering_kind        answering_model_name  \\\n",
       "0              20      prompting      gpt-4o-mini-2024-07-18   \n",
       "1              20      prompting           gpt-4o-2024-08-06   \n",
       "2              20      prompting     claude-3-haiku-20240307   \n",
       "3              20      prompting  claude-3-5-sonnet-20240620   \n",
       "4              20      prompting      gpt-4o-mini-2024-07-18   \n",
       "5              20      prompting           gpt-4o-2024-08-06   \n",
       "6              20      prompting     claude-3-haiku-20240307   \n",
       "7              20      prompting  claude-3-5-sonnet-20240620   \n",
       "\n",
       "              book_model_name answering_embedding_chunk  \\\n",
       "0  claude-3-5-sonnet-20240620                       n/a   \n",
       "1  claude-3-5-sonnet-20240620                       n/a   \n",
       "2  claude-3-5-sonnet-20240620                       n/a   \n",
       "3  claude-3-5-sonnet-20240620                       n/a   \n",
       "4           gpt-4o-2024-05-13                       n/a   \n",
       "5           gpt-4o-2024-05-13                       n/a   \n",
       "6           gpt-4o-2024-05-13                       n/a   \n",
       "7           gpt-4o-2024-05-13                       n/a   \n",
       "\n",
       "                                   evaluation_object  \n",
       "0  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "1  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "2  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "3  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "4  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "5  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "6  <epbench.src.evaluation.evaluation_wrapper.Eva...  \n",
       "7  <epbench.src.evaluation.evaluation_wrapper.Eva...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "from epbench.src.evaluation.precomputed_results import get_precomputed_results\n",
    "\n",
    "experiments = [\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'gpt-4o-mini-2024-07-18',     'book_model_name': 'claude-3-5-sonnet-20240620'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'gpt-4o-2024-08-06',          'book_model_name': 'claude-3-5-sonnet-20240620'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'claude-3-haiku-20240307',    'book_model_name': 'claude-3-5-sonnet-20240620'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'claude-3-5-sonnet-20240620', 'book_model_name': 'claude-3-5-sonnet-20240620'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'gpt-4o-mini-2024-07-18',     'book_model_name': 'gpt-4o-2024-05-13'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'gpt-4o-2024-08-06',          'book_model_name': 'gpt-4o-2024-05-13'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'claude-3-haiku-20240307',    'book_model_name': 'gpt-4o-2024-05-13'},\n",
    "    {'book_nb_events': 20, 'answering_kind': 'prompting', 'answering_model_name': 'claude-3-5-sonnet-20240620', 'book_model_name': 'gpt-4o-2024-05-13'},\n",
    "]\n",
    "\n",
    "for i in range(len(experiments)):\n",
    "    if not 'answering_embedding_chunk' in experiments[i]:\n",
    "        experiments[i]['answering_embedding_chunk'] = 'n/a'\n",
    "\n",
    "print(f\"{len(experiments)} experiments\")\n",
    "\n",
    "all_benchmarks = {'benchmark_claude_default_20': benchmark_claude_default_20,\n",
    "                  'benchmark_gpt_default_20': benchmark_gpt_default_20}\n",
    "\n",
    "df = get_precomputed_results(experiments, env_file, data_folder, all_benchmarks)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>bins_items_correct_answer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3-5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(prompting, gpt-4o-mini-2024-07-18, n/a)</th>\n",
       "      <td>0.73±0.44</td>\n",
       "      <td>0.91±0.26</td>\n",
       "      <td>0.82±0.25</td>\n",
       "      <td>0.87±0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(prompting, gpt-4o-2024-08-06, n/a)</th>\n",
       "      <td>0.88±0.33</td>\n",
       "      <td>0.92±0.24</td>\n",
       "      <td>0.87±0.20</td>\n",
       "      <td>0.82±0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(prompting, claude-3-haiku-20240307, n/a)</th>\n",
       "      <td>0.90±0.30</td>\n",
       "      <td>0.73±0.43</td>\n",
       "      <td>0.55±0.32</td>\n",
       "      <td>0.56±0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(prompting, claude-3-5-sonnet-20240620, n/a)</th>\n",
       "      <td>0.97±0.18</td>\n",
       "      <td>0.77±0.41</td>\n",
       "      <td>0.65±0.25</td>\n",
       "      <td>0.61±0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "bins_items_correct_answer                             0          1          2  \\\n",
       "count                                               150        150         47   \n",
       "(prompting, gpt-4o-mini-2024-07-18, n/a)      0.73±0.44  0.91±0.26  0.82±0.25   \n",
       "(prompting, gpt-4o-2024-08-06, n/a)           0.88±0.33  0.92±0.24  0.87±0.20   \n",
       "(prompting, claude-3-haiku-20240307, n/a)     0.90±0.30  0.73±0.43  0.55±0.32   \n",
       "(prompting, claude-3-5-sonnet-20240620, n/a)  0.97±0.18  0.77±0.41  0.65±0.25   \n",
       "\n",
       "bins_items_correct_answer                           3-5  \n",
       "count                                                21  \n",
       "(prompting, gpt-4o-mini-2024-07-18, n/a)      0.87±0.16  \n",
       "(prompting, gpt-4o-2024-08-06, n/a)           0.82±0.18  \n",
       "(prompting, claude-3-haiku-20240307, n/a)     0.56±0.27  \n",
       "(prompting, claude-3-5-sonnet-20240620, n/a)  0.61±0.15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def extract_groups(df, nb_events, relative_to, book_model_name = 'gpt-4o-2024-05-13', metric = 'f1_score_lenient'):\n",
    "\n",
    "    df_sliced = df[(df['book_nb_events'] == nb_events) & (df['book_model_name'] == book_model_name)]\n",
    "\n",
    "    # template\n",
    "    i = 0\n",
    "    df_res_0 = df_sliced.iloc[i]['evaluation_object'].get_pretty_summary_relative_to(relative_to, metric)\n",
    "    df_results = df_res_0.iloc[:, :-1] # take all but last column\n",
    "\n",
    "    # fill\n",
    "    for i in range(len(df_sliced)):\n",
    "        df_res_i = df_sliced.iloc[i]['evaluation_object'].get_pretty_summary_relative_to(relative_to, metric)\n",
    "        df_results[(df_sliced.iloc[i]['answering_kind'], \n",
    "                    df_sliced.iloc[i]['answering_model_name'],\n",
    "                    df_sliced.iloc[i]['answering_embedding_chunk'])] = [x for x in df_res_i.iloc[:, -1]] # average # [float(x.split('±')[0]) for x in df_res_i.iloc[:, -1]] # average\n",
    "\n",
    "    # remove the nan\n",
    "    df_results_tmp = df_results.copy()\n",
    "    for col in relative_to + ['count']:\n",
    "        df_results_tmp = df_results_tmp.loc[:, df_results_tmp.columns != col]\n",
    "    nan_rows = [[k for i, x in enumerate(df_results_tmp.iloc[k]) if np.isnan(float(x.split('±')[0]))==True ] for k in range(len(df_results))]\n",
    "    issue_rows = list(set([item for sublist in nan_rows for item in sublist]))\n",
    "    df_results = df_results.drop(issue_rows)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "nb_events = 20 # select the book of interest (either 20 or 200)\n",
    "relative_to = ['get', 'bins_items_correct_answer'] # select the grouped elements as a list among:\n",
    "# 'get': type of question, among 'all' (simple recall questions), 'latest' (latest state questions), or 'chronological' (chronological questions)\n",
    "# 'bins_items_correct_answer': number of events for this question, binned into {0}, {1}, {2}, {3,4,5}, {6+} chapters\n",
    "# 'cue': type of cue for this question, e.g. (*,*,*,c)\n",
    "# 'retrieval_type': type of trace for this question, e.g. 'Spaces'\n",
    "book_model_name = 'gpt-4o-2024-05-13' # showing only the new part of the table\n",
    "df_results = extract_groups(df, nb_events, relative_to, book_model_name) # group the results according to `relative_to`\n",
    "\n",
    "# Further filtering, e.g. for selecting only the simple recall questions:\n",
    "df_results = df_results[df_results['get'] == 'all'].drop('get', axis = 1)\n",
    "df_results.T.set_axis(df_results.T.iloc[0], axis=1).iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation Claude vs GPT books: one-sided Mann-Whitney U tests between pairs of models.\n",
      "When evaluated on the claude book, comparing gpt-4o-2024-08-06 vs claude-3-5-sonnet-20240620, we obtain a p-value of 0.1101\n",
      "When evaluated on the claude book, comparing gpt-4o-mini-2024-07-18 vs claude-3-haiku-20240307, we obtain a p-value of 0.5283\n",
      "When evaluated on the gpt book, comparing gpt-4o-2024-08-06 vs claude-3-5-sonnet-20240620, we obtain a p-value of 0.0003\n",
      "When evaluated on the gpt book, comparing gpt-4o-mini-2024-07-18 vs claude-3-haiku-20240307, we obtain a p-value of 0.0103\n"
     ]
    }
   ],
   "source": [
    "# Compute the statistical tests\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def get_f1_scores_vector(model_name, df_cur):\n",
    "    df_cur_model = df_cur[df_cur['answering_model_name'] == model_name]['evaluation_object']\n",
    "    if len(df_cur_model) != 1:\n",
    "        raise ValueError('only one element should be remaining with this model name')\n",
    "    df_generated_evaluations_cur = df_cur_model.iloc[0].df_generated_evaluations\n",
    "    a = np.array(df_generated_evaluations_cur['f1_score_lenient'].tolist())\n",
    "    return a # vector of scores\n",
    "\n",
    "print(\"Ablation Claude vs GPT books: one-sided Mann-Whitney U tests between pairs of models.\")\n",
    "\n",
    "for book_model_name in ['claude-3-5-sonnet-20240620', 'gpt-4o-2024-05-13']:\n",
    "    df_cur = df[df['book_model_name'] == book_model_name]\n",
    "    a_gpt_small = get_f1_scores_vector('gpt-4o-mini-2024-07-18', df_cur)\n",
    "    a_gpt_large = get_f1_scores_vector('gpt-4o-2024-08-06', df_cur)\n",
    "    a_claude_small = get_f1_scores_vector('claude-3-haiku-20240307', df_cur)\n",
    "    a_claude_large = get_f1_scores_vector('claude-3-5-sonnet-20240620', df_cur)\n",
    "\n",
    "    _, p_value_large = stats.mannwhitneyu(a_claude_large, a_gpt_large, alternative='less')\n",
    "    _, p_value_small = stats.mannwhitneyu(a_claude_small, a_gpt_small, alternative='less')\n",
    "\n",
    "    print(f'When evaluated on the {book_model_name.split('-')[0]} book, comparing gpt-4o-2024-08-06 vs claude-3-5-sonnet-20240620, we obtain a p-value of {round(p_value_large,4)}')\n",
    "    print(f'When evaluated on the {book_model_name.split('-')[0]} book, comparing gpt-4o-mini-2024-07-18 vs claude-3-haiku-20240307, we obtain a p-value of {round(p_value_small,4)}')\n",
    "\n",
    "#Ablation Claude vs GPT books: one-sided Mann-Whitney U tests between pairs of models.\n",
    "#When evaluated on the claude book, comparing gpt-4o-2024-08-06 vs claude-3-5-sonnet-20240620, we obtain a p-value of 0.1101\n",
    "#When evaluated on the claude book, comparing gpt-4o-mini-2024-07-18 vs claude-3-haiku-20240307, we obtain a p-value of 0.5283\n",
    "#When evaluated on the gpt book, comparing gpt-4o-2024-08-06 vs claude-3-5-sonnet-20240620, we obtain a p-value of 0.0003\n",
    "#When evaluated on the gpt book, comparing gpt-4o-mini-2024-07-18 vs claude-3-haiku-20240307, we obtain a p-value of 0.0103"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
