{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo_filepath = '/filepath/to/gitrepo/episodic-memory-benchmark'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0, 20.00% remaining with issues (4/20), for index: [11, 13, 16, 19].\n",
      "At iteration 1, 15.00% remaining with issues (3/20), for index: [11, 13, 16].\n",
      "At iteration 2, 10.00% remaining with issues (2/20), for index: [13, 16].\n",
      "At iteration 3, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 4, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 5, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 6, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 7, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At iteration 8, 5.00% remaining with issues (1/20), for index: [16].\n",
      "At final iteration 9, 5.00% remaining with issues (1/20), for index: [16].\n",
      "itermax reached but some events still did not pass the verification\n",
      "At iteration 0, 33.50% remaining with issues (67/200), for index: [11, 13, 16, 19, 20, 23, 25, 30, 33, 42, 44, 45, 47, 48, 50, 51, 56, 59, 62, 63, 67, 69, 70, 71, 79, 80, 85, 86, 88, 93, 96, 106, 109, 122, 125, 127, 128, 130, 136, 138, 143, 144, 146, 147, 148, 149, 150, 152, 155, 156, 160, 162, 163, 166, 169, 172, 175, 177, 178, 180, 181, 182, 185, 189, 193, 197, 199].\n",
      "At iteration 1, 16.50% remaining with issues (33/200), for index: [11, 13, 16, 42, 44, 56, 59, 67, 79, 80, 93, 96, 106, 122, 127, 128, 130, 136, 143, 144, 146, 147, 150, 156, 160, 162, 163, 166, 169, 172, 175, 182, 193].\n",
      "At iteration 2, 10.50% remaining with issues (21/200), for index: [13, 16, 42, 44, 56, 67, 79, 93, 96, 106, 143, 144, 146, 150, 156, 160, 162, 166, 169, 182, 193].\n",
      "At iteration 3, 7.50% remaining with issues (15/200), for index: [16, 42, 44, 56, 67, 93, 96, 106, 143, 144, 146, 156, 160, 182, 193].\n",
      "At iteration 4, 5.50% remaining with issues (11/200), for index: [16, 42, 44, 56, 67, 93, 146, 156, 160, 182, 193].\n",
      "At iteration 5, 4.50% remaining with issues (9/200), for index: [16, 56, 67, 93, 146, 156, 160, 182, 193].\n",
      "At iteration 6, 3.00% remaining with issues (6/200), for index: [16, 56, 67, 156, 160, 182].\n",
      "At iteration 7, 2.50% remaining with issues (5/200), for index: [16, 56, 67, 156, 160].\n",
      "At iteration 8, 2.00% remaining with issues (4/200), for index: [16, 56, 156, 160].\n",
      "At final iteration 9, 2.00% remaining with issues (4/200), for index: [16, 56, 156, 160].\n",
      "itermax reached but some events still did not pass the verification\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from epbench.src.generation.benchmark_generation_wrapper import BenchmarkGenerationWrapper\n",
    "book_parameters = {'indexing': 'default', 'nb_summaries': 0}\n",
    "data_folder = Path(git_repo_filepath) / 'epbench' / 'data'\n",
    "env_file = Path(git_repo_filepath) / '.env'\n",
    "\n",
    "# Generation with Claude -- 20 events\n",
    "prompt_parameters = {'nb_events': 20, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_20 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)\n",
    "\n",
    "# Generation with Claude -- 200 events\n",
    "prompt_parameters = {'nb_events': 200, 'name_universe': 'default', 'name_styles': 'default', 'seed': 0, 'distribution_events': {'name': 'geometric', 'param': 0.1}}\n",
    "model_parameters = {'model_name': 'claude-3-5-sonnet-20240620', 'max_new_tokens': 4096, 'itermax': 10}\n",
    "benchmark_claude_200 = BenchmarkGenerationWrapper(prompt_parameters, model_parameters, book_parameters, data_folder, env_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering 1: in-context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with 10397 tokens, answer with prompting using with gpt-4o-mini-2024-07-18\n",
      "Document with 10397 tokens, answer with prompting using with gpt-4o-2024-08-06\n",
      "Document with 10397 tokens, answer with prompting using with claude-3-haiku-20240307\n",
      "Document with 10397 tokens, answer with prompting using with claude-3-5-sonnet-20240620\n",
      "Document with 10397 tokens, answer with prompting using with o1-mini\n",
      "Document with 10397 tokens, answer with prompting using with llama-3.1-405b-instruct\n",
      "Document with 102870 tokens, answer with prompting using with gpt-4o-mini-2024-07-18\n",
      "Document with 102870 tokens, answer with prompting using with gpt-4o-2024-08-06\n",
      "Document with 102870 tokens, answer with prompting using with claude-3-haiku-20240307\n",
      "Document with 102870 tokens, answer with prompting using with claude-3-5-sonnet-20240620\n",
      "Document with 102870 tokens, answer with prompting using with o1-mini\n",
      "Document with 102870 tokens, answer with prompting using with llama-3.1-405b-instruct\n",
      "Experiment ended (prompting)\n"
     ]
    }
   ],
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "\n",
    "for my_benchmark in [benchmark_claude_20, benchmark_claude_200]:\n",
    "    for model_name in ['gpt-4o-mini-2024-07-18', 'gpt-4o-2024-08-06', 'claude-3-haiku-20240307', 'claude-3-5-sonnet-20240620', 'o1-mini', 'llama-3.1-405b-instruct']:\n",
    "        answering_parameters = {'kind': 'prompting', 'model_name': model_name, 'max_new_tokens': 4096, 'sleeping_time': 1, 'policy': 'remove_duplicates'}\n",
    "        print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with prompting using with {model_name}\")\n",
    "        my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Experiment ended (prompting)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering 2: RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with 10397 tokens, answer with rag using with gpt-4o-mini-2024-07-18 (paragraph chunks)\n",
      "Document with 10397 tokens, answer with rag using with gpt-4o-mini-2024-07-18 (chapter chunks)\n",
      "Document with 10397 tokens, answer with rag using with gpt-4o-2024-08-06 (paragraph chunks)\n",
      "Document with 10397 tokens, answer with rag using with gpt-4o-2024-08-06 (chapter chunks)\n",
      "Document with 10397 tokens, answer with rag using with claude-3-haiku-20240307 (paragraph chunks)\n",
      "Document with 10397 tokens, answer with rag using with claude-3-haiku-20240307 (chapter chunks)\n",
      "Document with 10397 tokens, answer with rag using with claude-3-5-sonnet-20240620 (paragraph chunks)\n",
      "Document with 10397 tokens, answer with rag using with claude-3-5-sonnet-20240620 (chapter chunks)\n",
      "Document with 102870 tokens, answer with rag using with gpt-4o-mini-2024-07-18 (paragraph chunks)\n",
      "Document with 102870 tokens, answer with rag using with gpt-4o-mini-2024-07-18 (chapter chunks)\n",
      "Document with 102870 tokens, answer with rag using with gpt-4o-2024-08-06 (paragraph chunks)\n",
      "Document with 102870 tokens, answer with rag using with gpt-4o-2024-08-06 (chapter chunks)\n",
      "Document with 102870 tokens, answer with rag using with claude-3-haiku-20240307 (paragraph chunks)\n",
      "Document with 102870 tokens, answer with rag using with claude-3-haiku-20240307 (chapter chunks)\n",
      "Document with 102870 tokens, answer with rag using with claude-3-5-sonnet-20240620 (paragraph chunks)\n",
      "Document with 102870 tokens, answer with rag using with claude-3-5-sonnet-20240620 (chapter chunks)\n",
      "Experiment ended (rag)\n"
     ]
    }
   ],
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "from epbench.src.evaluation.generator_answers_2_rag import get_top_n\n",
    "\n",
    "for my_benchmark in [benchmark_claude_20, benchmark_claude_200]:\n",
    "    for model_name in ['gpt-4o-mini-2024-07-18', 'gpt-4o-2024-08-06', 'claude-3-haiku-20240307', 'claude-3-5-sonnet-20240620']:\n",
    "        for embedding_chunk in ['paragraph', 'chapter']:\n",
    "            answering_parameters = {'kind': 'rag', \n",
    "                                    'model_name': model_name, \n",
    "                                    'embedding_chunk': embedding_chunk, \n",
    "                                    'max_new_tokens': 4096, \n",
    "                                    'sleeping_time': 0, \n",
    "                                    'embedding_model': 'text-embedding-3-small', \n",
    "                                    'embedding_batch_size': 2048, \n",
    "                                    'top_n': get_top_n(embedding_chunk, my_benchmark), \n",
    "                                    'policy': 'remove_duplicates'}\n",
    "            print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with rag using with {model_name} ({embedding_chunk} chunks)\")\n",
    "            my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Experiment ended (rag)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering 3: fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document with 10397 tokens, answer with ftuning using with gpt-4o-mini-2024-07-18\n",
      "Document with 102870 tokens, answer with ftuning using with gpt-4o-mini-2024-07-18\n",
      "Experiment ended (ftuning)\n"
     ]
    }
   ],
   "source": [
    "from epbench.src.evaluation.evaluation_wrapper import EvaluationWrapper\n",
    "\n",
    "for my_benchmark in [benchmark_claude_20, benchmark_claude_200]:\n",
    "    for model_name in ['gpt-4o-mini-2024-07-18']:\n",
    "        answering_parameters = {'kind': 'ftuning', \n",
    "                                'model_name': model_name, \n",
    "                                'max_new_tokens': 4096, \n",
    "                                'sleeping_time': 0, \n",
    "                                'ftuning_input_data_policy': 'single', \n",
    "                                'ftuning_need_upload': False, \n",
    "                                'ftuning_need_actual_tune': False, \n",
    "                                'batch_size': 'auto', \n",
    "                                'learning_rate_multiplier': 'auto', \n",
    "                                'n_epochs': 30,\n",
    "                                'policy': 'remove_duplicates'}\n",
    "        # ad-hoc selection of the fine-tuned models\n",
    "        if my_benchmark.nb_tokens() == 10397:\n",
    "            if model_name == 'gpt-4o-mini-2024-07-18':\n",
    "                answering_parameters['fine_tuned_model_name'] = 'ft:gpt-4o-mini-2024-07-18:personal::AAzm9XtH'\n",
    "            elif model_name == 'gpt-4o-2024-08-06':\n",
    "                answering_parameters['fine_tuned_model_name'] = 'ft:gpt-4o-2024-08-06:personal::AB02Cbei'\n",
    "            else:\n",
    "                raise ValueError('only done for gpt4o and gpt4o-mini')\n",
    "        elif my_benchmark.nb_tokens() == 102870:\n",
    "            if model_name == 'gpt-4o-mini-2024-07-18':\n",
    "                answering_parameters['fine_tuned_model_name'] = 'ft:gpt-4o-mini-2024-07-18:personal::AB0B6H4o'\n",
    "            elif model_name == 'gpt-4o-2024-08-06':\n",
    "                answering_parameters['fine_tuned_model_name'] = 'ft:gpt-4o-2024-08-06:personal::DISCARDED' # DISCARDED (~400 dollars)\n",
    "            else:\n",
    "                raise ValueError('only done for gpt4o and gpt4o-mini')\n",
    "        print(f\"Document with {my_benchmark.nb_tokens()} tokens, answer with ftuning using with {model_name}\")\n",
    "        my_evaluation = EvaluationWrapper(my_benchmark, answering_parameters, data_folder, env_file)\n",
    "\n",
    "print(\"Experiment ended (ftuning)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
